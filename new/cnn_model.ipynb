{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok ....\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv3D, Activation , Dense, AveragePooling3D, MaxPooling3D , Dropout, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from utils import *\n",
    "\n",
    "#declaring a model\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "#conv layer 1\n",
    "L = Conv2D(5,[4,4], data_format='channels_last', activation = 'relu', input_shape=(32,32,5))\n",
    "model.add(L)\n",
    "#pooling\n",
    "P = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', data_format='channels_last')    \n",
    "model.add(P)\n",
    "\n",
    "#conv layer 2\n",
    "output = 5\n",
    "L1 = Conv2D(5,[3,3], data_format='channels_last', activation = 'relu')\n",
    "model.add(L1)\n",
    "#pooling\n",
    "P1 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', data_format='channels_last')    \n",
    "model.add(P1)\n",
    "\"\"\"\n",
    "#full connected layer\n",
    "#model = model_nn(model, 26*26*5, 3, dropout=0.3, batch_normalization=True, activation='relu', neurons_decay=0, starting_power=1, l2=10**-5, compile_model=True, trainable=True)\n",
    "model.add(Dense(1, name=\"Dense_1\", activation='relu', input_shape=(32,32,5) ))\n",
    "#model.add(Dropout(rate=0.3, name='Dropout_1'))\n",
    "model.compile(loss=rmse_loss_keras, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "print(\"Ok ....\")\n",
    "#print(model.get_config())\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un objet de moins\n",
      "-------------- Data loading ok -----------\n",
      "     perte =  1\n",
      "1237648702973673741\n",
      "\n",
      "     number of images:  2599  class size : 2599\n",
      " -------- End -------\n",
      "Train on 1559 samples, validate on 520 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10, 1) for Tensor 'Dense_1_target:0', which has shape '(?, ?, ?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-25e68b12fad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_Valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test_valid_data_galaxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1113\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1114\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10, 1) for Tensor 'Dense_1_target:0', which has shape '(?, ?, ?, ?)'"
     ]
    }
   ],
   "source": [
    "batch_size = 10#128\n",
    "data = '../data/csvs/galaxies/g.csv';\n",
    "dir_img = '../data/images/galaxies1/all'\n",
    "\n",
    "X_Train, Y_Train, X_Test, Y_Test, X_Valid, Y_Valid = get_train_test_valid_data_galaxy(data,dir_img,test_size=0.4, valid_size = 0.5)\n",
    "\n",
    "history = model.fit(X_Train, Y_Train, validation_data=(X_Valid, Y_Valid), epochs=1, batch_size=batch_size, verbose=1)\n",
    "\n",
    "save_model(model, './model.json')\n",
    "\n",
    "plot_history(history);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]]\n",
      "[1 3 2]\n"
     ]
    }
   ],
   "source": [
    "plot_history_ac(history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X_Test, batch_size=batch_size).reshape(-1)\n",
    "metrics = metrics_with_object_class(Y_Test, predict, object_class_test, 'MLP')\n",
    "metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_quality(y_test, predict, object_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_ = '../data/csvs/galaxies/'\n",
    "path1 = dir_+'all1.csv'\n",
    "path2 = dir_+'g.csv'\n",
    "comp = 0;\n",
    "with open(path1) as f:\n",
    "    with open(path2,'w') as p2:\n",
    "        p2.write('')\n",
    "    with open(path2,'a') as p2:\n",
    "        for a in f.readlines():\n",
    "            p2.write(a)\n",
    "            if (comp == 2600):\n",
    "                break;\n",
    "            else:\n",
    "                comp = comp + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120\n"
     ]
    }
   ],
   "source": [
    "print(32*32*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/aaa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.path.join('../data/object_images/', str(int(values['objid'])))\n",
    "print(os.path.join('../data/','aaa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_csv = '../data/csvs/galaxies/g.csv';\n",
    "data = pd.read_csv(file_csv, usecols=['objid','redshift']);\n",
    "values = data['objid']\n",
    "perte = 0\n",
    "pert = ''\n",
    "for v in range(len(values)):\n",
    "    try:\n",
    "        suffix = 'npy'\n",
    "        path = os.path.join(dir_img, str(int(values[v]))+'.'+suffix)\n",
    "        np.load(path)\n",
    "        print('Ok')\n",
    "    except :\n",
    "        perte = perte + 1\n",
    "        pert = pert + str(int(values[v]))+\"\\n\"\n",
    "print(perte)\n",
    "print(pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1 2]\n",
      "   [1 4]]]\n",
      "\n",
      "\n",
      " [[[1 2]\n",
      "   [1 4]]]\n",
      "\n",
      "\n",
      " [[[1 2]\n",
      "   [1 4]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[1,2],[1,4]]])\n",
    "b = np.array([[[1,2],[1,4]]])\n",
    "c = np.array([[[1,2],[1,4]]])\n",
    "d = np.array([a,b,c]);\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un objet de moins\n",
      "-------------- Data loading ok -----------\n",
      "     perte =  1\n",
      "1237648702973673741\n",
      "\n",
      "     number of images:  2599  class size : 2599\n",
      " -------- End -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.17004971,  0.08659859,  0.12891068,  0.27372247,  0.64250898],\n",
       "         [ 0.16084211,  0.08659728,  0.17092466,  0.30864012,  0.76098377],\n",
       "         [ 0.17004971,  0.09321198,  0.08955635,  0.27369928,  0.3995885 ],\n",
       "         ..., \n",
       "         [ 0.19718702,  0.07655293,  0.12022147,  0.28876486,  0.8076697 ],\n",
       "         [ 0.18818912,  0.07989715,  0.15006608,  0.28876486,  0.66016912],\n",
       "         [ 0.22370492,  0.09317722,  0.13311359,  0.26855657,  0.71180147]],\n",
       "\n",
       "        [[ 0.15149648,  0.09650797,  0.1872026 ,  0.25835019,  0.5483923 ],\n",
       "         [ 0.22370492,  0.06985919,  0.14169197,  0.29382771,  0.62436789],\n",
       "         [ 0.22370492,  0.05967633,  0.19124363,  0.22157983,  0.67782503],\n",
       "         ..., \n",
       "         [ 0.18818912,  0.07653879,  0.14585482,  0.23744509,  0.60569179],\n",
       "         [ 0.19718702,  0.0731833 ,  0.17907125,  0.24269003,  0.52815509],\n",
       "         [ 0.21494351,  0.08987021,  0.10275091,  0.24269003,  0.48700911]],\n",
       "\n",
       "        [[ 0.19724965,  0.10305378,  0.16263336,  0.19441448,  0.60595822],\n",
       "         [ 0.23237813,  0.08990857,  0.14590757,  0.33762252,  0.90923464],\n",
       "         [ 0.15149648,  0.08659466,  0.12030265,  0.25833842,  0.74488592],\n",
       "         ..., \n",
       "         [ 0.19718702,  0.09317722,  0.17907125,  0.2787002 ,  0.7284658 ],\n",
       "         [ 0.23237813,  0.06980228,  0.17498063,  0.24791357,  0.50779366],\n",
       "         [ 0.24100074,  0.08323023,  0.15005293,  0.25310773,  0.82279533]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.20615357,  0.08989114,  0.19922701,  0.24251047,  0.7613259 ],\n",
       "         [ 0.27462727,  0.10629696,  0.11588236,  0.28858194,  0.79275388],\n",
       "         [ 0.25797892,  0.06983074,  0.1666972 ,  0.22665301,  0.72881919],\n",
       "         ..., \n",
       "         [ 0.19719955,  0.09973099,  0.1911428 ,  0.25804964,  0.66048449],\n",
       "         [ 0.24100074,  0.12561901,  0.08485757,  0.22660436,  0.62449867],\n",
       "         [ 0.16073817,  0.12878995,  0.13728729,  0.22660436,  0.79264337]],\n",
       "\n",
       "        [[ 0.22372931,  0.05624925,  0.16255555,  0.26324654,  0.79275388],\n",
       "         [ 0.2061528 ,  0.06306306,  0.12019441,  0.23196682,  0.83811802],\n",
       "         [ 0.24950208,  0.09977242,  0.15422043,  0.23725253,  0.52873075],\n",
       "         ..., \n",
       "         [ 0.24954963,  0.14148006,  0.15416811,  0.31809855,  0.56808984],\n",
       "         [ 0.24954963,  0.13515514,  0.14997414,  0.28853619,  0.79264337],\n",
       "         [ 0.17912227,  0.10952967,  0.17896917,  0.28853619,  0.76121187]],\n",
       "\n",
       "        [[ 0.24100074,  0.10629696,  0.1666972 ,  0.2529217 ,  0.72887814],\n",
       "         [ 0.2061528 ,  0.11277921,  0.10275091,  0.25291985,  0.62469476],\n",
       "         [ 0.24100074,  0.07991124,  0.14159265,  0.26323479,  0.52873075],\n",
       "         ..., \n",
       "         [ 0.21495582,  0.11275195,  0.16250367,  0.23718032,  0.66051602],\n",
       "         [ 0.16994672,  0.09973099,  0.15834458,  0.22660436,  0.62456405],\n",
       "         [ 0.26643124,  0.12561901,  0.07590254,  0.21589075,  0.82300973]]],\n",
       "\n",
       "\n",
       "       [[[ 0.25948679,  0.10985783,  0.16752347,  0.31159431,  0.65735805],\n",
       "         [ 0.23387676,  0.06334954,  0.16752347,  0.33098146,  0.62129027],\n",
       "         [ 0.2252166 ,  0.12594193,  0.19602123,  0.35933253,  0.80527186],\n",
       "         ..., \n",
       "         [ 0.21644391,  0.11631712,  0.16333342,  0.27668893,  0.67472911],\n",
       "         [ 0.17138757,  0.16047828,  0.17978539,  0.30662087,  0.77412844],\n",
       "         [ 0.17138757,  0.19391185,  0.17159323,  0.21932769,  0.50426114]],\n",
       "\n",
       "        [[ 0.21646848,  0.11951764,  0.16752347,  0.26142716,  0.62129027],\n",
       "         [ 0.17154182,  0.07350943,  0.20799026,  0.30666578,  0.75835687],\n",
       "         [ 0.15296414,  0.10332903,  0.20002684,  0.30175796,  0.48370332],\n",
       "         ..., \n",
       "         [ 0.18060093,  0.15118171,  0.15081434,  0.28173256,  0.86458617],\n",
       "         [ 0.14338563,  0.18492234,  0.12958111,  0.28173256,  0.72563374],\n",
       "         [ 0.20759359,  0.21461116,  0.11661591,  0.30171281,  0.60248923]],\n",
       "\n",
       "        [[ 0.18070282,  0.11310632,  0.20401648,  0.29678056,  0.80527186],\n",
       "         [ 0.20763853,  0.09678503,  0.19602123,  0.28681958,  0.54500055],\n",
       "         [ 0.24248651,  0.10985783,  0.19602123,  0.30666578,  0.725811  ],\n",
       "         ..., \n",
       "         [ 0.25107026,  0.14179792,  0.16747186,  0.27667737,  0.82021856],\n",
       "         [ 0.15285939,  0.15745854,  0.1550048 ,  0.33089378,  0.75824249],\n",
       "         [ 0.2252166 ,  0.15118171,  0.16747186,  0.26648384,  0.70895183]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.19875179,  0.10006239,  0.18380564,  0.27150783,  0.82064843],\n",
       "         [ 0.17154182,  0.10983049,  0.18380564,  0.28665921,  0.82064843],\n",
       "         [ 0.20764628,  0.09348301,  0.11226109,  0.28162894,  0.60289013],\n",
       "         ..., \n",
       "         [ 0.23392506,  0.13232522,  0.12520173,  0.28661337,  0.65735805],\n",
       "         [ 0.26792645,  0.10330151,  0.1380402 ,  0.28157136,  0.87888086],\n",
       "         [ 0.18967967,  0.14802846,  0.1507356 ,  0.3064861 ,  0.54500055]],\n",
       "\n",
       "        [[ 0.25102279,  0.10006239,  0.15912573,  0.22460732,  0.7093122 ],\n",
       "         [ 0.25948679,  0.12594193,  0.13809834,  0.32597157,  0.82064843],\n",
       "         [ 0.19875179,  0.10332903,  0.18380564,  0.26639032,  0.805381  ],\n",
       "         ..., \n",
       "         [ 0.2164562 ,  0.16047828,  0.1590997 ,  0.26633188,  0.63948679],\n",
       "         [ 0.2252166 ,  0.11951764,  0.13377455,  0.31623182,  0.75835687],\n",
       "         [ 0.2425344 ,  0.14802846,  0.16736861,  0.32111353,  0.83536875]],\n",
       "\n",
       "        [[ 0.16229613,  0.10006239,  0.15912573,  0.27657324,  0.62155259],\n",
       "         [ 0.19873929,  0.09677118,  0.16328157,  0.32111353,  0.79009843],\n",
       "         [ 0.23387676,  0.10006239,  0.12092479,  0.1865444 ,  0.63967997],\n",
       "         ..., \n",
       "         [ 0.2425344 ,  0.12911186,  0.12948728,  0.29159591,  0.70916206],\n",
       "         [ 0.19868925,  0.11951764,  0.12087071,  0.29159591,  0.725811  ],\n",
       "         [ 0.17143899,  0.14802846,  0.14650083,  0.2191316 ,  0.67491561]]],\n",
       "\n",
       "\n",
       "       [[[ 0.21525116,  0.12195185,  0.14399359,  0.29264411,  0.70292586],\n",
       "         [ 0.17056447,  0.0728074 ,  0.12270796,  0.26235488,  0.63315558],\n",
       "         [ 0.12384104,  0.08945869,  0.1397699 ,  0.24164803,  0.6149745 ],\n",
       "         ..., \n",
       "         [ 0.2152327 ,  0.06266183,  0.18522668,  0.12534982,  0.75227714],\n",
       "         [ 0.24109666,  0.11228837,  0.19330834,  0.28762102,  0.53846741],\n",
       "         [ 0.19760036,  0.07949118,  0.13126612,  0.26235488,  0.78376329]],\n",
       "\n",
       "        [[ 0.1976254 ,  0.08615334,  0.09656339,  0.27759105,  0.70292586],\n",
       "         [ 0.20647091,  0.0626905 ,  0.16483557,  0.25720054,  0.81439644],\n",
       "         [ 0.17968339,  0.08283   ,  0.15238783,  0.23640062,  0.47705877],\n",
       "         ..., \n",
       "         [ 0.27462727,  0.07615001,  0.15238783,  0.31230918,  0.87298357],\n",
       "         [ 0.2064523 ,  0.06943223,  0.1397699 ,  0.28261861,  0.76814413],\n",
       "         [ 0.2064523 ,  0.05587415,  0.21707046,  0.28261861,  0.55821973]],\n",
       "\n",
       "        [[ 0.15212575,  0.09603679,  0.10536357,  0.29264411,  0.68591899],\n",
       "         [ 0.15212575,  0.07950176,  0.12700418,  0.22050335,  0.55821973],\n",
       "         [ 0.21525116,  0.0927601 ,  0.18522668,  0.20977332,  0.59632218],\n",
       "         ..., \n",
       "         [ 0.2064523 ,  0.09930278,  0.16068619,  0.24685912,  0.71964836],\n",
       "         [ 0.27462727,  0.09274619,  0.18522668,  0.28762102,  0.66868037],\n",
       "         [ 0.23257162,  0.08614634,  0.17303172,  0.32199857,  0.71964836]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.2325958 ,  0.08285809,  0.14402002,  0.21510351,  0.84401405],\n",
       "         [ 0.20648953,  0.0795273 ,  0.10974846,  0.20972383,  0.71947002],\n",
       "         [ 0.21526346,  0.0455763 ,  0.14402002,  0.24681143,  0.82920831],\n",
       "         ..., \n",
       "         [ 0.20647091,  0.10911931,  0.16896781,  0.2724916 ,  0.6858421 ],\n",
       "         [ 0.21525116,  0.12836057,  0.14821267,  0.19882685,  0.65098023],\n",
       "         [ 0.21524501,  0.13467515,  0.19330834,  0.22045439,  0.75216204]],\n",
       "\n",
       "        [[ 0.17968339,  0.06609561,  0.14823899,  0.2775448 ,  0.47705877],\n",
       "         [ 0.20648953,  0.08948659,  0.13556162,  0.25715333,  0.57731539],\n",
       "         [ 0.2325958 ,  0.08948659,  0.10539103,  0.20431505,  0.55808002],\n",
       "         ..., \n",
       "         [ 0.18866937,  0.11234292,  0.1270176 ,  0.25715333,  0.75216204],\n",
       "         [ 0.23257162,  0.13467515,  0.13979644,  0.29755142,  0.63309079],\n",
       "         [ 0.26633772,  0.09933041,  0.14821267,  0.20969909,  0.76803088]],\n",
       "\n",
       "        [[ 0.1976254 ,  0.05930252,  0.11408698,  0.30738416,  0.51831704],\n",
       "         [ 0.22397329,  0.07283578,  0.18121214,  0.23107709,  0.61477643],\n",
       "         [ 0.14275084,  0.09935804,  0.16488734,  0.25199574,  0.78365183],\n",
       "         ..., \n",
       "         [ 0.16136165,  0.08948659,  0.14821267,  0.32676429,  0.70280498],\n",
       "         [ 0.23257162,  0.092774  ,  0.16073817,  0.28752947,  0.84401405],\n",
       "         [ 0.2239489 ,  0.11234292,  0.19732481,  0.22576463,  0.63309079]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.22893728,  0.09881218,  0.11597748,  0.23633738,  0.57262719],\n",
       "         [ 0.20655158,  0.09330232,  0.08620233,  0.20982899,  0.519189  ],\n",
       "         [ 0.19516169,  0.09054643,  0.11597408,  0.18258601,  0.57262719],\n",
       "         ..., \n",
       "         [ 0.18941456,  0.07380711,  0.15704131,  0.22578898,  0.42321038],\n",
       "         [ 0.20085101,  0.08219071,  0.07312657,  0.19355984,  0.63974434],\n",
       "         [ 0.17784582,  0.10153507,  0.1159571 ,  0.1714904 ,  0.60675704]],\n",
       "\n",
       "        [[ 0.14829162,  0.08499774,  0.13259253,  0.22581333,  0.53729117],\n",
       "         [ 0.20655158,  0.07943217,  0.13259253,  0.20445187,  0.68727058],\n",
       "         [ 0.18365327,  0.11783729,  0.13259253,  0.18811327,  0.70262361],\n",
       "         ..., \n",
       "         [ 0.18364057,  0.12051909,  0.09905405,  0.2415282 ,  0.63974434],\n",
       "         [ 0.17200443,  0.07380711,  0.13259253,  0.18808798,  0.62338674],\n",
       "         [ 0.20085101,  0.11242474,  0.09905405,  0.16587023,  0.76178193]],\n",
       "\n",
       "        [[ 0.17200443,  0.1070033 ,  0.06435158,  0.22052783,  0.46271396],\n",
       "         [ 0.18365327,  0.10154887,  0.05993504,  0.23633738,  0.57263577],\n",
       "         [ 0.17784582,  0.09880526,  0.12846793,  0.21519579,  0.67167813],\n",
       "         ..., \n",
       "         [ 0.25078532,  0.07380711,  0.12430604,  0.22050335,  0.58984619],\n",
       "         [ 0.17784582,  0.13120171,  0.12014028,  0.22580115,  0.48189554],\n",
       "         [ 0.17784582,  0.10698959,  0.1159571 ,  0.19900194,  0.55512398]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.19516169,  0.10698959,  0.07754184,  0.25717694,  0.62351757],\n",
       "         [ 0.18365327,  0.10698959,  0.14491804,  0.21531883,  0.36086535],\n",
       "         [ 0.1362742 ,  0.11781016,  0.11599786,  0.18823968,  0.67180282],\n",
       "         ..., \n",
       "         [ 0.1894114 ,  0.11781016,  0.13672681,  0.20455137,  0.68739337],\n",
       "         [ 0.21217048,  0.1124111 ,  0.14897548,  0.25201946,  0.55522907],\n",
       "         [ 0.19514914,  0.09603679,  0.13672681,  0.29727942,  0.62345219]],\n",
       "\n",
       "        [[ 0.17784582,  0.10426605,  0.11600126,  0.22594723,  0.55524659],\n",
       "         [ 0.21217048,  0.12586121,  0.11600126,  0.23645784,  0.5899477 ],\n",
       "         [ 0.17200443,  0.08219071,  0.09481623,  0.20457624,  0.4431572 ],\n",
       "         ..., \n",
       "         [ 0.17200443,  0.09603679,  0.10332903,  0.28248057,  0.60689014],\n",
       "         [ 0.17200443,  0.10151785,  0.12432625,  0.27740607,  0.48204631],\n",
       "         [ 0.22888874,  0.09327453,  0.10332903,  0.22592288,  0.63987309]],\n",
       "\n",
       "        [[ 0.18365327,  0.06817874,  0.11600126,  0.23648193,  0.57274765],\n",
       "         [ 0.2008635 ,  0.12586121,  0.14084406,  0.24688296,  0.63987309],\n",
       "         [ 0.18942088,  0.09879144,  0.11179727,  0.22595941,  0.5899477 ],\n",
       "         ..., \n",
       "         [ 0.16021831,  0.13382126,  0.12016734,  0.17718142,  0.57274765],\n",
       "         [ 0.17200443,  0.07658119,  0.14897548,  0.19371071,  0.40285754],\n",
       "         [ 0.19514914,  0.11510748,  0.08188146,  0.25712973,  0.5899477 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.15424658,  0.10388619,  0.09880526,  0.20115054,  0.55300218],\n",
       "         [ 0.16016631,  0.14148006,  0.1364606 ,  0.26409054,  0.63781101],\n",
       "         [ 0.18352628,  0.11476742,  0.16483557,  0.19026014,  0.55296707],\n",
       "         ..., \n",
       "         [ 0.14222156,  0.10931084,  0.12818609,  0.20108815,  0.62142146],\n",
       "         [ 0.16012731,  0.10111798,  0.12402976,  0.21186806,  0.58774596],\n",
       "         [ 0.14222156,  0.10111798,  0.11985608,  0.2435396 ,  0.42064872]],\n",
       "\n",
       "        [[ 0.20072618,  0.10661248,  0.17677233,  0.22258186,  0.51691437],\n",
       "         [ 0.1422745 ,  0.11205649,  0.11151069,  0.19026014,  0.44064623],\n",
       "         [ 0.17774363,  0.10661248,  0.172775  ,  0.19026014,  0.44064623],\n",
       "         ..., \n",
       "         [ 0.11783729,  0.09010032,  0.14865991,  0.23831114,  0.46017459],\n",
       "         [ 0.18352628,  0.09562086,  0.1072432 ,  0.20649883,  0.65393621],\n",
       "         [ 0.18352628,  0.08732159,  0.09874997,  0.19020967,  0.57050771]],\n",
       "\n",
       "        [[ 0.16016631,  0.09563819,  0.11151069,  0.21193287,  0.44064623],\n",
       "         [ 0.17774363,  0.07332511,  0.11151069,  0.2065578 ,  0.6047588 ],\n",
       "         [ 0.18352628,  0.07895287,  0.12821294,  0.19572017,  0.57050771],\n",
       "         ..., \n",
       "         [ 0.16602857,  0.10931084,  0.08154398,  0.23310354,  0.62142146],\n",
       "         [ 0.22869453,  0.08174086,  0.10299872,  0.20110062,  0.42048842],\n",
       "         [ 0.17771809,  0.10658505,  0.10299872,  0.17917329,  0.49830866]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.1422745 ,  0.12014028,  0.172775  ,  0.19038628,  0.55265105],\n",
       "         [ 0.19501731,  0.09008638,  0.16483557,  0.16260743,  0.51658905],\n",
       "         [ 0.10544596,  0.08730761,  0.11990345,  0.17937736,  0.47917771],\n",
       "         ..., \n",
       "         [ 0.15419428,  0.12548442,  0.11147657,  0.21734062,  0.49801195],\n",
       "         [ 0.18352628,  0.09007243,  0.12818609,  0.25396255,  0.62102795],\n",
       "         [ 0.16012731,  0.09284354,  0.09019792,  0.20661674,  0.58740681]],\n",
       "\n",
       "        [[ 0.20635298,  0.07612881,  0.11151752,  0.24888363,  0.53477365],\n",
       "         [ 0.18352628,  0.09562432,  0.10305378,  0.2385035 ,  0.55265105],\n",
       "         [ 0.1719145 ,  0.12284292,  0.09880526,  0.16260743,  0.24363528],\n",
       "         ..., \n",
       "         [ 0.20072618,  0.12814583,  0.13643397,  0.23320024,  0.60435867],\n",
       "         [ 0.18352628,  0.08730761,  0.09874997,  0.22265515,  0.49797484],\n",
       "         [ 0.16602212,  0.12278893,  0.10299872,  0.25911003,  0.47910213]],\n",
       "\n",
       "        [[ 0.13013053,  0.12284292,  0.10729803,  0.24368311,  0.60449207],\n",
       "         [ 0.15422043,  0.09008638,  0.11572097,  0.18489698,  0.3997522 ],\n",
       "         [ 0.1719145 ,  0.12284292,  0.15275462,  0.22270399,  0.70068705],\n",
       "         ..., \n",
       "         [ 0.20635298,  0.12548442,  0.09448309,  0.25911003,  0.49797484],\n",
       "         [ 0.14821267,  0.07048509,  0.14871252,  0.21734062,  0.49797484],\n",
       "         [ 0.11168811,  0.09284354,  0.12818609,  0.18484624,  0.60435867]]],\n",
       "\n",
       "\n",
       "       [[[ 0.18937668,  0.10637929,  0.11967672,  0.24912155,  0.41968641],\n",
       "         [ 0.19511776,  0.10637929,  0.0813752 ,  0.20692696,  0.45940381],\n",
       "         [ 0.16613197,  0.08991903,  0.12799816,  0.2439701 ,  0.55215919],\n",
       "         ..., \n",
       "         [ 0.12421845,  0.08990508,  0.1196598 ,  0.1741605 ,  0.49756673],\n",
       "         [ 0.17793521,  0.09541629,  0.12382755,  0.28963327,  0.49756673],\n",
       "         [ 0.09938567,  0.09541629,  0.13622095,  0.19617173,  0.53432661]],\n",
       "\n",
       "        [[ 0.1542989 ,  0.13055915,  0.12799816,  0.19609648,  0.49752963],\n",
       "         [ 0.14232743,  0.1144953 ,  0.13213806,  0.15724994,  0.45932671],\n",
       "         [ 0.1542989 ,  0.12524213,  0.10706499,  0.21230623,  0.35711432],\n",
       "         ..., \n",
       "         [ 0.21219516,  0.11715893,  0.13622095,  0.22304589,  0.53430873],\n",
       "         [ 0.15440351,  0.10090074,  0.08131894,  0.13446175,  0.58699965],\n",
       "         [ 0.13034487,  0.11715893,  0.09855641,  0.19071418,  0.51610875]],\n",
       "\n",
       "        [[ 0.1362742 ,  0.0759874 ,  0.13624758,  0.17405793,  0.47857279],\n",
       "         [ 0.1542989 ,  0.14639539,  0.14444272,  0.23874389,  0.63703656],\n",
       "         [ 0.21209642,  0.13055915,  0.11548989,  0.1796324 ,  0.41968641],\n",
       "         ..., \n",
       "         [ 0.1424333 ,  0.0731833 ,  0.10280599,  0.24921669,  0.66918063],\n",
       "         [ 0.17210722,  0.13579476,  0.12798473,  0.1741605 ,  0.55215919],\n",
       "         [ 0.15442966,  0.10906459,  0.12382755,  0.26455912,  0.47864842]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.13018413,  0.07878365,  0.13624758,  0.25948679,  0.58666027],\n",
       "         [ 0.17198516,  0.07034287,  0.10706499,  0.1741605 ,  0.6367783 ],\n",
       "         [ 0.11179727,  0.0871398 ,  0.14444272,  0.20704481,  0.49719554],\n",
       "         ..., \n",
       "         [ 0.17209437,  0.11446808,  0.13211131,  0.24931183,  0.56940311],\n",
       "         [ 0.16623858,  0.07595911,  0.09428871,  0.22842741,  0.6203717 ],\n",
       "         [ 0.18372945,  0.10633813,  0.10705128,  0.20172438,  0.58666027]],\n",
       "\n",
       "        [[ 0.12411063,  0.10636556,  0.0813752 ,  0.22837885,  0.6203717 ],\n",
       "         [ 0.11794578,  0.09816919,  0.09000271,  0.20163709,  0.56940311],\n",
       "         [ 0.16612874,  0.0675514 ,  0.16063422,  0.2388881 ,  0.51571929],\n",
       "         ..., \n",
       "         [ 0.12421845,  0.08712581,  0.13211131,  0.23893616,  0.47834581],\n",
       "         [ 0.12421845,  0.07595911,  0.16058224,  0.24411353,  0.49723268],\n",
       "         [ 0.17210722,  0.08712581,  0.1444163 ,  0.23893616,  0.37807518]],\n",
       "\n",
       "        [[ 0.1542989 ,  0.1036386 ,  0.1070787 ,  0.25438857,  0.45894107],\n",
       "         [ 0.16612551,  0.0926697 ,  0.11129229,  0.2544359 ,  0.49715844],\n",
       "         [ 0.18937668,  0.10907827,  0.12385452,  0.19622189,  0.53391516],\n",
       "         ..., \n",
       "         [ 0.1424333 ,  0.09264884,  0.09428871,  0.23370767,  0.55184293],\n",
       "         [ 0.1894777 ,  0.09264884,  0.09428871,  0.23370767,  0.55184293],\n",
       "         [ 0.2721197 ,  0.1090509 ,  0.11966657,  0.19629714,  0.56940311]]]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "data = '../data/csvs/galaxies/g.csv';\n",
    "dir_img = '../data/images/galaxies1/all'\n",
    "X, Y =  load_data(data, dir_img)\n",
    "#X = np.array(X)\n",
    "#Y = np.array(Y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [np.array([1,2]), np.array([3,4]), np.array([4,5])]\n",
    "b = np.array(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2], [3,4], [4,5]]\n",
    "b = np.stack(a,axis = 0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 32, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = []\n",
    "Y = []\n",
    "VX = []\n",
    "VY = []\n",
    "for i in range(100):\n",
    "    X.append(np.random.randn(32, 32, 2))\n",
    "    Y.append(np.random.randn(32)[0])\n",
    "    VX.append(np.random.randn(32, 32, 2))\n",
    "    VY.append(np.random.randn(32)[1])\n",
    "print(np.array(Y).shape)\n",
    "print(np.array(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_29 to have 3 dimensions, but got array with shape (1000, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3868436d28b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m model.fit(x_train, y_train,\n\u001b[0;32m---> 45\u001b[0;31m           epochs=4,batch_size=128)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m#score = model.evaluate(x_test, y_test, batch_size=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_29 to have 3 dimensions, but got array with shape (1000, 10)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.random.random((1000, 20,3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20,3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "\"\"\"\n",
    "X = []\n",
    "Y = []\n",
    "VX = []\n",
    "VY = []\n",
    "for i in range(100):\n",
    "    X.append(np.random.randn(32, 32, 2))\n",
    "    Y.append(np.random.randn(32)[0])\n",
    "    VX.append(np.random.randn(32, 32, 2))\n",
    "    VY.append(np.random.randn(32)[1])\n",
    "x_train = np.array(X)\n",
    "y_train = np.array(Y)\n",
    "x_test = np.array(VX)\n",
    "y_test = np.array(VY) \"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(20,3,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=4,batch_size=128)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.random.random((1000, 20, 3))\n",
    "a =k.shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
